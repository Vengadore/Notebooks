{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training optic Discs in Efficientnet.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vengadore/Notebooks/blob/master/Training_optic_Discs_in_Efficientnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVm5Ats_YVEZ"
      },
      "source": [
        "# Diabetic Retinopathy Detection\n",
        "\n",
        "Kaggle has a large competition of Diabetic Retinopathy detection which can be found here:\n",
        "https://www.kaggle.com/c/diabetic-retinopathy-detection/\n",
        "\n",
        "Their dataset consists in 35126 images labeled from 0 to 4 according to the degree of Retinopathy.\n",
        "An analysis of the data is provided in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QABUN5czaGrr",
        "outputId": "5fe48356-1f32-4b6b-8d57-bf29207ac76a"
      },
      "source": [
        "!rm -rf sample_data\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb 19 14:04:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrtmQaiBbcy5"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKfg7kcTbb4h"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install efficientnet_pytorch\n",
        "clear_output(wait=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO_cEyuIeDjq"
      },
      "source": [
        "## Load data from local drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWk0Ox2ueHNB",
        "outputId": "951981f0-b9f6-4f38-f2e0-e816ae937d71"
      },
      "source": [
        "!wget -O File.zip \"https://correoipn-my.sharepoint.com/:u:/g/personal/ccarrillog1400_alumno_ipn_mx/ETdcB8GscyBAkhUFvNMzpoUBLLgg7ej9Q3t4rK_bR8ngSA?download=1\"\n",
        "!unzip File.zip\n",
        "!rm File.zip\n",
        "clear_output(wait=False)\n",
        "print(\"Data Downloaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUCY7oQLorC9"
      },
      "source": [
        "!rm checkpoint*"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dNqkjvcUetED",
        "outputId": "7b14ccae-13fb-4730-e736-5020212c1dcb"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "Bad =  pd.DataFrame({\"imageFilename\":[os.path.join(\"./OpticDiscs/Neovessels\",File) for File in os.listdir(\"./OpticDiscs/Neovessels\") if \".jpeg\" in File]})\n",
        "Good = pd.DataFrame({\"imageFilename\":[os.path.join(\"./OpticDiscs/Good images\",File) for File in os.listdir(\"./OpticDiscs/Good images\") if \".jpeg\" in File]}).sample(len(Bad),random_state = 42)\n",
        "\n",
        "data = pd.concat((Good,Bad),axis = 0)\n",
        "data.head()\n",
        "data['class'] = data['imageFilename'].apply(lambda x : 1 if \"Good\" in x else 0)\n",
        "data = data.reset_index()\n",
        "data = data[['imageFilename','class']]\n",
        "data.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageFilename</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./OpticDiscs/Good images/4270_left.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./OpticDiscs/Good images/26079_left.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./OpticDiscs/Good images/4998_right.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./OpticDiscs/Good images/21026_right.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./OpticDiscs/Good images/40631_left.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               imageFilename  class\n",
              "0    ./OpticDiscs/Good images/4270_left.jpeg      1\n",
              "1   ./OpticDiscs/Good images/26079_left.jpeg      1\n",
              "2   ./OpticDiscs/Good images/4998_right.jpeg      1\n",
              "3  ./OpticDiscs/Good images/21026_right.jpeg      1\n",
              "4   ./OpticDiscs/Good images/40631_left.jpeg      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3N0Ue5MCvlk",
        "outputId": "d481c4c8-fca9-49cc-aeef-9c2554d23ffb"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deKSOu-Kd8Yk"
      },
      "source": [
        "#### Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwL1cUG_oX7U"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "Train,Validation = train_test_split(data,test_size = 0.40,random_state = 65)\r\n",
        "X_train = Train['imageFilename']\r\n",
        "y_train = Train['class']\r\n",
        "#Split data\r\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(Validation['imageFilename'], Validation['class'], test_size=0.50, random_state=65)\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfHkcbVKogxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5becdf-668b-4b70-f3c4-c650b5b5432c"
      },
      "source": [
        "print(\"Datos de entrenamiento:\")\r\n",
        "print(y_train.value_counts())\r\n",
        "print(\"Datos de validación:\")\r\n",
        "print(y_validation.value_counts())\r\n",
        "print(\"Datos de test:\")\r\n",
        "print(y_test.value_counts())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datos de entrenamiento:\n",
            "0    119\n",
            "1    105\n",
            "Name: class, dtype: int64\n",
            "Datos de validación:\n",
            "1    44\n",
            "0    31\n",
            "Name: class, dtype: int64\n",
            "Datos de test:\n",
            "1    38\n",
            "0    37\n",
            "Name: class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVIy3dn7eKFq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0c3190d8-1b4a-4e7e-eff4-528358ab5939"
      },
      "source": [
        "test_data = pd.DataFrame({'image':X_test,'class':y_test})\n",
        "test_data['image'] = test_data['image'].apply(lambda x : x.split('/')[-1])\n",
        "test_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>37829_left.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>29712_left.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>32085_right.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>11896_left.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>21096_left.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                image  class\n",
              "198   37829_left.jpeg      0\n",
              "9     29712_left.jpeg      1\n",
              "132  32085_right.jpeg      1\n",
              "259   11896_left.jpeg      0\n",
              "208   21096_left.jpeg      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YsYT7VjerXq"
      },
      "source": [
        "test_data.to_csv('test_data.csv',index=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPh3qDdIdkHh"
      },
      "source": [
        "## Definition of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-FUmuJidoy3",
        "outputId": "c3a321ec-9f51-429c-bda0-d281efffeaad"
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "import torch\n",
        "network_name = 'efficientnet-b7'\n",
        "model = EfficientNet.from_pretrained(network_name)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drpW39Qwj3l9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6fccc4-e8f1-4b3f-b5df-3a4851f7497c"
      },
      "source": [
        "model._fc"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=2560, out_features=1000, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu_aoH7-tBsT"
      },
      "source": [
        "## Change efficientnet final layer\n",
        "model._fc = torch.nn.Linear(in_features=2560,out_features=2,bias = True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNns-LOy6TrK",
        "outputId": "6399f79d-c9df-4e0a-e677-5bc5e1ebe7d1"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "count_parameters(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63792082"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BR_Hs12gAqx"
      },
      "source": [
        "from torchvision.transforms import Resize,ToTensor,Compose,Normalize\n",
        "from torchvision.transforms import RandomHorizontalFlip,RandomVerticalFlip,RandomRotation,ColorJitter,RandomResizedCrop,RandomPerspective,GaussianBlur\n",
        "from PIL import Image\n",
        "\n",
        "transforms = Compose([RandomHorizontalFlip(),RandomVerticalFlip(),\n",
        "                      RandomRotation(180),ColorJitter(0.5,0.5,0.5),\n",
        "                      RandomResizedCrop((600,600), scale=(0.7, 1.2)),\n",
        "                      RandomPerspective(),\n",
        "                      GaussianBlur((3,3))]) # Transformations for the training images\n",
        "\n",
        "composed = Compose([Resize(600), # Resize to a fit size for efficientnet\n",
        "                    ToTensor(),  # Convert into sensor\n",
        "                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Normalize image"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWg1p7EQtqVZ"
      },
      "source": [
        "### Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiT-MlPutsZR"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device);\n",
        "seed = 17\n",
        "torch.manual_seed(seed)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJWflI4JoVRW"
      },
      "source": [
        "try:\n",
        "    model = torch.load('./checkpoint_efficientnet-b7_12_0.89333.ph', map_location=device)\n",
        "except:\n",
        "    print(\"No Checkpoint loaded\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcTzzyr5uUdr"
      },
      "source": [
        "classes = {0:[0,0,0,0,1],\n",
        "           1:[0,0,0,1,0],\n",
        "           2:[0,0,1,0,0],\n",
        "           3:[0,1,0,0,0],\n",
        "           4:[1,0,0,0,0]}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "RkvlQWRQtjXo",
        "outputId": "8d9d5dd8-3f7a-45a0-bc9f-eb681cc3c618"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "random.seed(seed)\n",
        "\n",
        "epochs = 80\n",
        "batch_size = 1  # I will use batch size of 1 to keep the ratio of each image\n",
        "\n",
        "TRAINING_acc = []\n",
        "VALIDATION_acc = []\n",
        "BEST_val_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    indexes = [idx for idx in range(len(X_train))]\n",
        "    pbar = tqdm( range(len(X_train)//batch_size),ncols = 100)\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    t = 0\n",
        "\n",
        "    for step in pbar:\n",
        "        # Load data\n",
        "        idx = random.sample(indexes,batch_size)\n",
        "        X = X_train.iloc[idx]\n",
        "        y = y_train.iloc[idx]\n",
        "\n",
        "        # Remove indexes\n",
        "        [indexes.remove(i) for i in idx]\n",
        "\n",
        "        # Load images\n",
        "        try:\n",
        "            images = [Image.open(File) for File in X]\n",
        "        except:\n",
        "            continue\n",
        "        # Load y_true\n",
        "        y_true = torch.LongTensor([c for c in y]).to(device)\n",
        "        \n",
        "        # Convert images to tensor\n",
        "        x_batch = torch.FloatTensor().to(device)\n",
        "        for image in images:\n",
        "            P = transforms(image)\n",
        "            P = composed(P).unsqueeze(0).to(device)\n",
        "            x_batch = torch.cat((x_batch,P))\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_true)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        t += batch_size\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_acc += torch.sum(preds == y_true).cpu().detach().numpy()\n",
        "        acc = torch.sum(preds == y_true).cpu().detach().numpy()/batch_size;\n",
        "        pbar.set_description(\"Epoch: {} Accuracy: {:0.5f} Loss: {:0.5f} \".format(epoch+1,running_acc/t,loss.item()))\n",
        "    #Validation\n",
        "    TRAINING_acc.append(running_acc/t)\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "    t = 0\n",
        "    for point in range(len(X_validation)//batch_size):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            X = X_validation.iloc[point*batch_size:(point+1)*batch_size]\n",
        "            y = y_validation.iloc[point*batch_size:(point+1)*batch_size]\n",
        "\n",
        "\n",
        "            # Load images\n",
        "            try:\n",
        "                images = [Image.open(File) for File in X]\n",
        "            except:\n",
        "                continue\n",
        "            # Load y_true\n",
        "            y_true = torch.LongTensor([c for c in y]).to(device)\n",
        "            \n",
        "            # Convert images to tensor\n",
        "            x_batch = torch.FloatTensor().to(device)\n",
        "            for image in images:\n",
        "                P = composed(image).unsqueeze(0).to(device)\n",
        "                x_batch = torch.cat((x_batch,P))\n",
        "\n",
        "            \n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_true)\n",
        "            val_loss += loss.item()\n",
        "            t += batch_size\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_acc += torch.sum(preds == y_true).cpu().detach().numpy()\n",
        "    VALIDATION_acc.append(val_acc/t)\n",
        "    print(\"Validation -- Accuracy: {:0.5f} Loss: {:0.5f} \\n\".format(val_acc/t,loss.item()))\n",
        "    if val_acc/t > BEST_val_acc:\n",
        "        try:\n",
        "            torch.save(model,\"/content/checkpoint_{}_{}_{:0.5f}.ph\".format(network_name,epoch+1,val_acc/t))\n",
        "            BEST_val_acc = val_acc/t\n",
        "        except:\n",
        "            continue"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Accuracy: 0.91518 Loss: 0.06654 : 100%|██████████████████| 224/224 [02:04<00:00,  1.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.86667 Loss: 0.07141 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2 Accuracy: 0.91964 Loss: 0.09552 : 100%|██████████████████| 224/224 [02:04<00:00,  1.80it/s]\n",
            "  0%|                                                                       | 0/224 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.84000 Loss: 0.14195 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3 Accuracy: 0.93304 Loss: 0.03395 : 100%|██████████████████| 224/224 [02:05<00:00,  1.79it/s]\n",
            "  0%|                                                                       | 0/224 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.81333 Loss: 0.06557 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4 Accuracy: 0.95982 Loss: 0.06235 : 100%|██████████████████| 224/224 [02:05<00:00,  1.79it/s]\n",
            "  0%|                                                                       | 0/224 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.84000 Loss: 0.05273 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5 Accuracy: 0.95982 Loss: 0.98572 : 100%|██████████████████| 224/224 [02:04<00:00,  1.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.89333 Loss: 0.18445 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6 Accuracy: 0.95982 Loss: 0.04255 : 100%|██████████████████| 224/224 [02:04<00:00,  1.80it/s]\n",
            "  0%|                                                                       | 0/224 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.84000 Loss: 0.15982 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7 Accuracy: 0.98214 Loss: 0.02990 : 100%|██████████████████| 224/224 [02:04<00:00,  1.79it/s]\n",
            "  0%|                                                                       | 0/224 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation -- Accuracy: 0.84000 Loss: 0.17255 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8 Accuracy: 0.99219 Loss: 0.03605 :  57%|██████████▎       | 128/224 [01:11<00:53,  1.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f231aad2aeef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpupTOMPlWD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "92773ce9-f855-4dca-ece4-d4a40df55a1e"
      },
      "source": [
        "history = pd.DataFrame({\"Train\":TRAINING_acc,\"Validation\":VALIDATION_acc})\n",
        "history.plot()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f01d1f4d7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PV_uLtQnaO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a79e1e-a6f5-4493-b4d9-f0a3f5730bd9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint_efficientnet-b7_1_0.60000.ph\n",
            "checkpoint_efficientnet-b7_12_0.90667.ph\n",
            "checkpoint_efficientnet-b7_15_0.92000.ph\n",
            "checkpoint_efficientnet-b7_4_0.74667.ph\n",
            "checkpoint_efficientnet-b7_44_0.93333.ph\n",
            "checkpoint_efficientnet-b7_5_0.77333.ph\n",
            "checkpoint_efficientnet-b7_6_0.80000.ph\n",
            "checkpoint_efficientnet-b7_7_0.82667.ph\n",
            "checkpoint_efficientnet-b7_8_0.88000.ph\n",
            "OpticDiscs\n",
            "test_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VumKHi2Snf4F"
      },
      "source": [
        "torch.save(model,\"checkpoint.ph\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYI16QYGyqLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f849c5-c8a0-43c7-ac44-1130ba6bd5a6"
      },
      "source": [
        "for c in y:\n",
        "  print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv2fODvHJlIM"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ennB90UVVmb"
      },
      "source": [
        "model = torch.load('checkpoint_efficientnet-b7_5_0.89333.ph')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLd2H4FrJnXo"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6nVWUqYJoDh"
      },
      "source": [
        "predicted = []\n",
        "true_values = []\n",
        "all_predicted = []\n",
        "all_true = []\n",
        "for point in range(len(X_test)//batch_size):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        X = X_test.iloc[point*batch_size:(point+1)*batch_size]\n",
        "        y = y_test.iloc[point*batch_size:(point+1)*batch_size]\n",
        "\n",
        "        true_values.append(y.values)\n",
        "        # Load images\n",
        "        try:\n",
        "            images = [Image.open(File) for File in X]\n",
        "        except:\n",
        "            continue\n",
        "        # Load y_true\n",
        "        y_true = torch.LongTensor([c for c in y]).to(device)\n",
        "        \n",
        "        # Convert images to tensor\n",
        "        x_batch = torch.FloatTensor().to(device)\n",
        "        for image in images:\n",
        "            P = composed(image).unsqueeze(0).to(device)\n",
        "            x_batch = torch.cat((x_batch,P))\n",
        "\n",
        "        \n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_true)\n",
        "        all_predicted.append(outputs.cpu().detach().numpy()[0][1])\n",
        "        all_true.append(y_true.cpu().detach().numpy()[0])\n",
        "        \n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predicted.append(preds)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwUWGj_IKoup"
      },
      "source": [
        "true_values = np.array(true_values).reshape(-1)\n",
        "dummy = torch.FloatTensor().to(device)\n",
        "for tensor in predicted:\n",
        "    dummy = torch.cat((dummy,tensor))\n",
        "predicted = dummy.view(-1).cpu().numpy()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjxNUd7jk_-j",
        "outputId": "715206c9-a29c-4f1e-dc9c-5b1e6a39e511"
      },
      "source": [
        "confusion_matrix(true_values, predicted)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[33,  4],\n",
              "       [ 7, 31]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKAd7jpBo_07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b32978b-1928-46c7-85c5-c224331e1a5a"
      },
      "source": [
        "from sklearn.metrics import roc_curve\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "ns_fpr, ns_tpr, _ = roc_curve(all_true, all_predicted)\r\n",
        "plt.plot(ns_fpr,ns_tpr)\r\n",
        "plt.xlabel(\"FPR\")\r\n",
        "plt.ylabel(\"TPR\")\r\n",
        "print(roc_auc_score(all_true, all_predicted))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2768, -1.3801]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en2UTHBIxio5"
      },
      "source": [
        "!mkdir Results\r\n",
        "roc_data = pd.DataFrame(data = {\"ns_fpr\":ns_fpr,\"ns_tpr\":ns_tpr})\r\n",
        "roc_data.to_csv(\"./Results/\"+network_name+\".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}