{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "DCGAN_OPTOS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vengadore/Notebooks/blob/master/DCGAN_OPTOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9oGTBMg4y52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7d9d76b-9303-47a8-c19a-59fe9a21a9cc"
      },
      "source": [
        "# Download dataset\n",
        "from IPython.display import clear_output\n",
        "!wget \"https://correoipn-my.sharepoint.com/:u:/g/personal/ccarrillog1400_alumno_ipn_mx/ERn7rS35L3hAg6MH6ktwR4wBJImUoXbLnGS_AHzjFYP65g?download=1\"\n",
        "!sudo chmod 777 \"ERn7rS35L3hAg6MH6ktwR4wBJImUoXbLnGS_AHzjFYP65g?download=1\"\n",
        "!mv \"ERn7rS35L3hAg6MH6ktwR4wBJImUoXbLnGS_AHzjFYP65g?download=1\" \"Light_Images.zip\"\n",
        "!unzip Light_Images.zip\n",
        "!rm Light_Images.zip\n",
        "!rm OPTOS_Light_Images.csv\n",
        "clear_output(wait=False)\n",
        "\n",
        "!wget \"https://correoipn-my.sharepoint.com/:x:/g/personal/ccarrillog1400_alumno_ipn_mx/ETQVuYc90PpGktGtEF867g4BCo_28KZdv76nwJ-I_XF4QA?download=1\"\n",
        "!sudo chmod 777 \"ETQVuYc90PpGktGtEF867g4BCo_28KZdv76nwJ-I_XF4QA?download=1\"\n",
        "!mv \"ETQVuYc90PpGktGtEF867g4BCo_28KZdv76nwJ-I_XF4QA?download=1\" \"OPTOS_Light_Images.csv\"\n",
        "clear_output(wait=False)\n",
        "\n",
        "print(\"Data downloaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM1B_gBd56fO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "3a1090cc-9162-4be5-a96d-1e7814fe480c"
      },
      "source": [
        "!git clone \"https://github.com/Vengadore/Segmentation_OPTOS.git\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Segmentation_OPTOS'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 111 (delta 43), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (111/111), 76.88 MiB | 11.26 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6M9MNID407v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Segmentation_OPTOS.GAN_OPTOS import dcgan_OPTOS as dcgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S13LhOF4y5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98c023bc-5db9-4b8c-f439-905cddba0649"
      },
      "source": [
        "## Create the network\n",
        "Net = dcgan.DCGAN(img_shape=(224,224,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 112, 112, 32)      896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 112, 112, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 57, 57, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 57, 57, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 57, 57, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 57, 57, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 29, 29, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 29, 29, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 29, 29, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 29, 29, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 29, 29, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 29, 29, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 29, 29, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 215296)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 215297    \n",
            "=================================================================\n",
            "Total params: 605,505\n",
            "Trainable params: 604,609\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 56, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 56, 56, 3)         1731      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 56, 56, 3)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 112, 112, 3)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 224, 224, 3)       0         \n",
            "=================================================================\n",
            "Total params: 857,347\n",
            "Trainable params: 856,963\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z31Qmvi34y6J",
        "colab_type": "text"
      },
      "source": [
        "## Get data with dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXPlNArk4y6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path = \"./\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPsmIj6p4y6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "63115552-fb0a-4da2-fea1-cc8ab6f8f8f9"
      },
      "source": [
        "data = pd.read_csv(os.path.join(path,\"OPTOS_Light_Images.csv\"))\n",
        "print(len(data))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageFilename</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x_width</th>\n",
              "      <th>y_width</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000.jpeg</td>\n",
              "      <td>2187.0</td>\n",
              "      <td>1395.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001.jpeg</td>\n",
              "      <td>1438.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002.jpeg</td>\n",
              "      <td>1865.0</td>\n",
              "      <td>2372.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003.jpeg</td>\n",
              "      <td>2272.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004.jpeg</td>\n",
              "      <td>1726.0</td>\n",
              "      <td>1211.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  imageFilename       x       y  x_width  y_width Class\n",
              "0     0000.jpeg  2187.0  1395.0    209.0    178.0   Bad\n",
              "1     0001.jpeg  1438.0  1643.0    136.0    145.0   Bad\n",
              "2     0002.jpeg  1865.0  2372.0    185.0    190.0   Bad\n",
              "3     0003.jpeg  2272.0  1484.0    177.0    169.0   Bad\n",
              "4     0004.jpeg  1726.0  1211.0    185.0    194.0   Bad"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc0aMcNa4y6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "c5dac631-d013-4aad-a917-7b7a5fdf0967"
      },
      "source": [
        "data = data.dropna()\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageFilename</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x_width</th>\n",
              "      <th>y_width</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000.jpeg</td>\n",
              "      <td>2187.0</td>\n",
              "      <td>1395.0</td>\n",
              "      <td>209.0</td>\n",
              "      <td>178.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001.jpeg</td>\n",
              "      <td>1438.0</td>\n",
              "      <td>1643.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002.jpeg</td>\n",
              "      <td>1865.0</td>\n",
              "      <td>2372.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003.jpeg</td>\n",
              "      <td>2272.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004.jpeg</td>\n",
              "      <td>1726.0</td>\n",
              "      <td>1211.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>1129.jpeg</td>\n",
              "      <td>1804.0</td>\n",
              "      <td>1540.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>1131.jpeg</td>\n",
              "      <td>1158.0</td>\n",
              "      <td>1855.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>1132.jpeg</td>\n",
              "      <td>1722.0</td>\n",
              "      <td>2474.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1133</th>\n",
              "      <td>1133.jpeg</td>\n",
              "      <td>2490.0</td>\n",
              "      <td>2462.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1135</th>\n",
              "      <td>1135.jpeg</td>\n",
              "      <td>939.0</td>\n",
              "      <td>1474.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>Bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>962 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     imageFilename       x       y  x_width  y_width Class\n",
              "0        0000.jpeg  2187.0  1395.0    209.0    178.0   Bad\n",
              "1        0001.jpeg  1438.0  1643.0    136.0    145.0   Bad\n",
              "2        0002.jpeg  1865.0  2372.0    185.0    190.0   Bad\n",
              "3        0003.jpeg  2272.0  1484.0    177.0    169.0   Bad\n",
              "4        0004.jpeg  1726.0  1211.0    185.0    194.0   Bad\n",
              "...            ...     ...     ...      ...      ...   ...\n",
              "1129     1129.jpeg  1804.0  1540.0    116.0    106.0   Bad\n",
              "1131     1131.jpeg  1158.0  1855.0    172.0    197.0   Bad\n",
              "1132     1132.jpeg  1722.0  2474.0    135.0    139.0   Bad\n",
              "1133     1133.jpeg  2490.0  2462.0    174.0    201.0   Bad\n",
              "1135     1135.jpeg   939.0  1474.0    143.0    139.0   Bad\n",
              "\n",
              "[962 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNQN8jdq4y6b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "87524202-f0ce-49b6-ede0-6b79931c4b51"
      },
      "source": [
        "data['imageFilename'] = data['imageFilename'].apply(lambda x: os.path.join(path,os.path.join('Light_Images',x)))\n",
        "data['Class'] = data['Class'].apply(lambda x: int(x == \"Good\"))\n",
        "data = data[data['Class']==1]\n",
        "print(len(data))\n",
        "data = data.sample(600,random_state = 42)\n",
        "print(len(data))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "641\n",
            "600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imageFilename</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x_width</th>\n",
              "      <th>y_width</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>./Light_Images/0424.jpeg</td>\n",
              "      <td>1468.0</td>\n",
              "      <td>1546.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>./Light_Images/0648.jpeg</td>\n",
              "      <td>1737.0</td>\n",
              "      <td>2055.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>./Light_Images/0788.jpeg</td>\n",
              "      <td>1724.0</td>\n",
              "      <td>2701.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>./Light_Images/0656.jpeg</td>\n",
              "      <td>1293.0</td>\n",
              "      <td>2398.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>./Light_Images/0410.jpeg</td>\n",
              "      <td>1317.0</td>\n",
              "      <td>1459.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                imageFilename       x       y  x_width  y_width  Class\n",
              "424  ./Light_Images/0424.jpeg  1468.0  1546.0    139.0    134.0      1\n",
              "648  ./Light_Images/0648.jpeg  1737.0  2055.0    131.0    143.0      1\n",
              "788  ./Light_Images/0788.jpeg  1724.0  2701.0    165.0    103.0      1\n",
              "656  ./Light_Images/0656.jpeg  1293.0  2398.0    153.0    166.0      1\n",
              "410  ./Light_Images/0410.jpeg  1317.0  1459.0    164.0    145.0      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbk2h8fl7sva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bc780937-a37b-49d0-9423-badd51086bab"
      },
      "source": [
        "!mkdir images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘images’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6hst1Tl4y6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1446628-ab3f-46c2-d0f8-19976f00b7a0"
      },
      "source": [
        "Net.train(data,X = 'imageFilename',epochs = 50000,batch_size = 256, save_interval= 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.841722, acc.: 50.20%] [G loss: 0.732891]\n",
            "1 [D loss: 0.834197, acc.: 48.83%] [G loss: 0.759682]\n",
            "2 [D loss: 0.776011, acc.: 53.52%] [G loss: 0.794334]\n",
            "3 [D loss: 0.773504, acc.: 53.91%] [G loss: 0.825037]\n",
            "4 [D loss: 0.779424, acc.: 52.93%] [G loss: 0.853261]\n",
            "5 [D loss: 0.794385, acc.: 51.37%] [G loss: 0.831760]\n",
            "6 [D loss: 0.781033, acc.: 51.95%] [G loss: 0.882668]\n",
            "7 [D loss: 0.791892, acc.: 50.20%] [G loss: 0.909595]\n",
            "8 [D loss: 0.830612, acc.: 50.00%] [G loss: 0.904329]\n",
            "9 [D loss: 0.761604, acc.: 54.49%] [G loss: 0.951253]\n",
            "10 [D loss: 0.799942, acc.: 51.56%] [G loss: 0.921353]\n",
            "11 [D loss: 0.816667, acc.: 50.59%] [G loss: 1.081284]\n",
            "12 [D loss: 0.764758, acc.: 54.30%] [G loss: 1.021126]\n",
            "13 [D loss: 0.789432, acc.: 50.39%] [G loss: 0.994426]\n",
            "14 [D loss: 0.827221, acc.: 50.00%] [G loss: 1.025049]\n",
            "15 [D loss: 0.823244, acc.: 49.61%] [G loss: 0.999008]\n",
            "16 [D loss: 0.748988, acc.: 55.47%] [G loss: 1.002466]\n",
            "17 [D loss: 0.814502, acc.: 51.76%] [G loss: 0.960452]\n",
            "18 [D loss: 0.796214, acc.: 54.10%] [G loss: 0.879893]\n",
            "19 [D loss: 0.781933, acc.: 52.73%] [G loss: 0.929706]\n",
            "20 [D loss: 0.772650, acc.: 53.52%] [G loss: 1.005182]\n",
            "21 [D loss: 0.801585, acc.: 51.37%] [G loss: 0.935933]\n",
            "22 [D loss: 0.785421, acc.: 54.69%] [G loss: 0.927517]\n",
            "23 [D loss: 0.820647, acc.: 49.41%] [G loss: 0.959569]\n",
            "24 [D loss: 0.818206, acc.: 50.59%] [G loss: 0.852289]\n",
            "25 [D loss: 0.795108, acc.: 53.52%] [G loss: 0.959178]\n",
            "26 [D loss: 0.790727, acc.: 55.08%] [G loss: 0.929239]\n",
            "27 [D loss: 0.811404, acc.: 50.59%] [G loss: 1.023179]\n",
            "28 [D loss: 0.788923, acc.: 54.49%] [G loss: 0.939535]\n",
            "29 [D loss: 0.834539, acc.: 50.98%] [G loss: 1.039326]\n",
            "30 [D loss: 0.852467, acc.: 50.39%] [G loss: 0.904300]\n",
            "31 [D loss: 0.879098, acc.: 46.09%] [G loss: 0.929093]\n",
            "32 [D loss: 0.830841, acc.: 51.95%] [G loss: 0.942003]\n",
            "33 [D loss: 0.803607, acc.: 53.91%] [G loss: 0.888444]\n",
            "34 [D loss: 0.816305, acc.: 51.95%] [G loss: 0.929371]\n",
            "35 [D loss: 0.788629, acc.: 53.32%] [G loss: 0.898289]\n",
            "36 [D loss: 0.769650, acc.: 51.37%] [G loss: 0.910171]\n",
            "37 [D loss: 0.748288, acc.: 57.23%] [G loss: 0.911586]\n",
            "38 [D loss: 0.816908, acc.: 50.20%] [G loss: 0.928666]\n",
            "39 [D loss: 0.814392, acc.: 50.78%] [G loss: 0.956477]\n",
            "40 [D loss: 0.848584, acc.: 48.44%] [G loss: 0.942413]\n",
            "41 [D loss: 0.797489, acc.: 50.98%] [G loss: 0.849859]\n",
            "42 [D loss: 0.802700, acc.: 52.34%] [G loss: 0.839723]\n",
            "43 [D loss: 0.822736, acc.: 51.95%] [G loss: 0.945146]\n",
            "44 [D loss: 0.857492, acc.: 46.09%] [G loss: 0.931592]\n",
            "45 [D loss: 0.813870, acc.: 50.00%] [G loss: 0.897039]\n",
            "46 [D loss: 0.848757, acc.: 47.27%] [G loss: 0.939434]\n",
            "47 [D loss: 0.839583, acc.: 48.05%] [G loss: 0.890208]\n",
            "48 [D loss: 0.767871, acc.: 55.27%] [G loss: 0.982847]\n",
            "49 [D loss: 0.828445, acc.: 48.63%] [G loss: 1.006578]\n",
            "50 [D loss: 0.824812, acc.: 48.24%] [G loss: 0.858605]\n",
            "51 [D loss: 0.785936, acc.: 53.12%] [G loss: 0.901277]\n",
            "52 [D loss: 0.815896, acc.: 50.20%] [G loss: 0.973871]\n",
            "53 [D loss: 0.806336, acc.: 51.17%] [G loss: 0.880245]\n",
            "54 [D loss: 0.812350, acc.: 50.98%] [G loss: 0.861381]\n",
            "55 [D loss: 0.831935, acc.: 48.63%] [G loss: 0.907275]\n",
            "56 [D loss: 0.849961, acc.: 50.00%] [G loss: 0.895210]\n",
            "57 [D loss: 0.868672, acc.: 46.09%] [G loss: 0.969543]\n",
            "58 [D loss: 0.813322, acc.: 48.44%] [G loss: 0.968109]\n",
            "59 [D loss: 0.788624, acc.: 55.27%] [G loss: 0.956939]\n",
            "60 [D loss: 0.819337, acc.: 52.54%] [G loss: 1.008593]\n",
            "61 [D loss: 0.783057, acc.: 51.37%] [G loss: 0.919961]\n",
            "62 [D loss: 0.788923, acc.: 53.12%] [G loss: 0.932716]\n",
            "63 [D loss: 0.817837, acc.: 49.22%] [G loss: 0.934248]\n",
            "64 [D loss: 0.787371, acc.: 55.08%] [G loss: 1.003173]\n",
            "65 [D loss: 0.788454, acc.: 55.08%] [G loss: 0.930809]\n",
            "66 [D loss: 0.793062, acc.: 51.17%] [G loss: 0.949799]\n",
            "67 [D loss: 0.775217, acc.: 53.12%] [G loss: 0.967098]\n",
            "68 [D loss: 0.809879, acc.: 49.61%] [G loss: 0.893963]\n",
            "69 [D loss: 0.803749, acc.: 51.56%] [G loss: 0.858170]\n",
            "70 [D loss: 0.782431, acc.: 53.12%] [G loss: 0.940921]\n",
            "71 [D loss: 0.803913, acc.: 49.02%] [G loss: 0.931435]\n",
            "72 [D loss: 0.819165, acc.: 49.80%] [G loss: 0.865404]\n",
            "73 [D loss: 0.796183, acc.: 53.71%] [G loss: 0.926074]\n",
            "74 [D loss: 0.765962, acc.: 56.25%] [G loss: 0.929814]\n",
            "75 [D loss: 0.775577, acc.: 54.30%] [G loss: 0.894671]\n",
            "76 [D loss: 0.803765, acc.: 51.76%] [G loss: 0.930664]\n",
            "77 [D loss: 0.786016, acc.: 52.34%] [G loss: 0.923804]\n",
            "78 [D loss: 0.819079, acc.: 48.44%] [G loss: 0.914408]\n",
            "79 [D loss: 0.777451, acc.: 53.52%] [G loss: 0.867743]\n",
            "80 [D loss: 0.832595, acc.: 49.41%] [G loss: 0.934063]\n",
            "81 [D loss: 0.792112, acc.: 53.32%] [G loss: 0.867745]\n",
            "82 [D loss: 0.814920, acc.: 52.54%] [G loss: 0.996431]\n",
            "83 [D loss: 0.816162, acc.: 51.17%] [G loss: 0.876713]\n",
            "84 [D loss: 0.799077, acc.: 50.20%] [G loss: 0.944483]\n",
            "85 [D loss: 0.773860, acc.: 52.54%] [G loss: 0.968275]\n",
            "86 [D loss: 0.767343, acc.: 54.88%] [G loss: 0.906733]\n",
            "87 [D loss: 0.803097, acc.: 52.34%] [G loss: 0.947332]\n",
            "88 [D loss: 0.817229, acc.: 50.00%] [G loss: 0.908042]\n",
            "89 [D loss: 0.836818, acc.: 47.85%] [G loss: 0.914241]\n",
            "90 [D loss: 0.803361, acc.: 52.34%] [G loss: 0.911916]\n",
            "91 [D loss: 0.831157, acc.: 51.17%] [G loss: 0.875552]\n",
            "92 [D loss: 0.766936, acc.: 56.84%] [G loss: 0.880649]\n",
            "93 [D loss: 0.778700, acc.: 51.17%] [G loss: 0.906903]\n",
            "94 [D loss: 0.827042, acc.: 50.78%] [G loss: 0.933681]\n",
            "95 [D loss: 0.751463, acc.: 51.95%] [G loss: 0.972692]\n",
            "96 [D loss: 0.819643, acc.: 50.39%] [G loss: 0.903242]\n",
            "97 [D loss: 0.831623, acc.: 49.80%] [G loss: 0.996486]\n",
            "98 [D loss: 0.777993, acc.: 54.69%] [G loss: 0.919055]\n",
            "99 [D loss: 0.800341, acc.: 53.52%] [G loss: 0.874064]\n",
            "100 [D loss: 0.827626, acc.: 50.78%] [G loss: 1.006429]\n",
            "101 [D loss: 0.817855, acc.: 50.78%] [G loss: 0.911201]\n",
            "102 [D loss: 0.803051, acc.: 51.76%] [G loss: 0.909217]\n",
            "103 [D loss: 0.846386, acc.: 50.59%] [G loss: 0.999040]\n",
            "104 [D loss: 0.817110, acc.: 48.24%] [G loss: 0.982332]\n",
            "105 [D loss: 0.821723, acc.: 48.24%] [G loss: 0.951768]\n",
            "106 [D loss: 0.810219, acc.: 49.02%] [G loss: 1.013726]\n",
            "107 [D loss: 0.815456, acc.: 52.34%] [G loss: 0.875595]\n",
            "108 [D loss: 0.798127, acc.: 54.69%] [G loss: 0.891706]\n",
            "109 [D loss: 0.753065, acc.: 53.71%] [G loss: 0.880059]\n",
            "110 [D loss: 0.801659, acc.: 52.15%] [G loss: 0.930593]\n",
            "111 [D loss: 0.872030, acc.: 46.09%] [G loss: 0.934101]\n",
            "112 [D loss: 0.812235, acc.: 50.98%] [G loss: 0.958562]\n",
            "113 [D loss: 0.813741, acc.: 52.15%] [G loss: 0.958807]\n",
            "114 [D loss: 0.800082, acc.: 52.73%] [G loss: 0.952633]\n",
            "115 [D loss: 0.794002, acc.: 53.52%] [G loss: 0.985041]\n",
            "116 [D loss: 0.795808, acc.: 51.76%] [G loss: 0.882153]\n",
            "117 [D loss: 0.791660, acc.: 50.39%] [G loss: 0.903966]\n",
            "118 [D loss: 0.775435, acc.: 52.73%] [G loss: 0.941761]\n",
            "119 [D loss: 0.828601, acc.: 50.78%] [G loss: 0.941516]\n",
            "120 [D loss: 0.800367, acc.: 52.15%] [G loss: 0.906286]\n",
            "121 [D loss: 0.799955, acc.: 52.34%] [G loss: 0.973785]\n",
            "122 [D loss: 0.813155, acc.: 50.98%] [G loss: 0.904329]\n",
            "123 [D loss: 0.808584, acc.: 51.95%] [G loss: 0.882508]\n",
            "124 [D loss: 0.809590, acc.: 47.85%] [G loss: 0.905882]\n",
            "125 [D loss: 0.842713, acc.: 47.46%] [G loss: 1.003816]\n",
            "126 [D loss: 0.799675, acc.: 52.54%] [G loss: 0.909838]\n",
            "127 [D loss: 0.813984, acc.: 50.59%] [G loss: 0.875354]\n",
            "128 [D loss: 0.842782, acc.: 48.63%] [G loss: 0.928653]\n",
            "129 [D loss: 0.812341, acc.: 50.39%] [G loss: 0.895209]\n",
            "130 [D loss: 0.800765, acc.: 51.76%] [G loss: 0.984236]\n",
            "131 [D loss: 0.790053, acc.: 50.78%] [G loss: 0.912955]\n",
            "132 [D loss: 0.790437, acc.: 53.12%] [G loss: 0.901559]\n",
            "133 [D loss: 0.775546, acc.: 54.49%] [G loss: 0.882363]\n",
            "134 [D loss: 0.768633, acc.: 53.52%] [G loss: 0.995469]\n",
            "135 [D loss: 0.810197, acc.: 48.63%] [G loss: 0.872316]\n",
            "136 [D loss: 0.778096, acc.: 54.49%] [G loss: 0.895098]\n",
            "137 [D loss: 0.813252, acc.: 50.20%] [G loss: 0.950803]\n",
            "138 [D loss: 0.768095, acc.: 56.45%] [G loss: 0.876844]\n",
            "139 [D loss: 0.800898, acc.: 52.73%] [G loss: 0.888228]\n",
            "140 [D loss: 0.798275, acc.: 51.95%] [G loss: 0.916694]\n",
            "141 [D loss: 0.752190, acc.: 55.47%] [G loss: 0.916516]\n",
            "142 [D loss: 0.788159, acc.: 53.71%] [G loss: 0.964883]\n",
            "143 [D loss: 0.760749, acc.: 56.05%] [G loss: 0.893639]\n",
            "144 [D loss: 0.802963, acc.: 50.98%] [G loss: 0.965996]\n",
            "145 [D loss: 0.762655, acc.: 55.47%] [G loss: 0.891229]\n",
            "146 [D loss: 0.797464, acc.: 54.69%] [G loss: 0.963232]\n",
            "147 [D loss: 0.817156, acc.: 52.93%] [G loss: 0.923398]\n",
            "148 [D loss: 0.825067, acc.: 50.00%] [G loss: 0.892512]\n",
            "149 [D loss: 0.818583, acc.: 51.56%] [G loss: 0.963567]\n",
            "150 [D loss: 0.811990, acc.: 49.80%] [G loss: 0.932012]\n",
            "151 [D loss: 0.772308, acc.: 52.93%] [G loss: 0.917794]\n",
            "152 [D loss: 0.809538, acc.: 49.80%] [G loss: 0.962210]\n",
            "153 [D loss: 0.804547, acc.: 51.17%] [G loss: 0.942963]\n",
            "154 [D loss: 0.793675, acc.: 51.37%] [G loss: 0.927714]\n",
            "155 [D loss: 0.792659, acc.: 50.59%] [G loss: 0.831910]\n",
            "156 [D loss: 0.814232, acc.: 51.17%] [G loss: 1.000777]\n",
            "157 [D loss: 0.829664, acc.: 50.59%] [G loss: 1.011672]\n",
            "158 [D loss: 0.789629, acc.: 53.32%] [G loss: 1.022181]\n",
            "159 [D loss: 0.788693, acc.: 53.12%] [G loss: 0.882257]\n",
            "160 [D loss: 0.765261, acc.: 54.30%] [G loss: 0.905804]\n",
            "161 [D loss: 0.803189, acc.: 51.56%] [G loss: 0.954902]\n",
            "162 [D loss: 0.835058, acc.: 48.63%] [G loss: 0.938949]\n",
            "163 [D loss: 0.834795, acc.: 49.41%] [G loss: 0.946122]\n",
            "164 [D loss: 0.817171, acc.: 51.56%] [G loss: 0.894111]\n",
            "165 [D loss: 0.786581, acc.: 53.32%] [G loss: 0.887497]\n",
            "166 [D loss: 0.751224, acc.: 53.32%] [G loss: 0.874332]\n",
            "167 [D loss: 0.795304, acc.: 51.37%] [G loss: 0.835747]\n",
            "168 [D loss: 0.780931, acc.: 51.76%] [G loss: 0.886715]\n",
            "169 [D loss: 0.808231, acc.: 51.95%] [G loss: 0.938644]\n",
            "170 [D loss: 0.795988, acc.: 52.54%] [G loss: 0.942746]\n",
            "171 [D loss: 0.813886, acc.: 51.37%] [G loss: 0.927988]\n",
            "172 [D loss: 0.814004, acc.: 52.34%] [G loss: 0.901096]\n",
            "173 [D loss: 0.782143, acc.: 55.66%] [G loss: 0.960826]\n",
            "174 [D loss: 0.802026, acc.: 51.17%] [G loss: 0.909921]\n",
            "175 [D loss: 0.787777, acc.: 52.15%] [G loss: 0.883069]\n",
            "176 [D loss: 0.772616, acc.: 53.52%] [G loss: 0.860093]\n",
            "177 [D loss: 0.781967, acc.: 53.32%] [G loss: 0.956049]\n",
            "178 [D loss: 0.817019, acc.: 51.17%] [G loss: 0.782291]\n",
            "179 [D loss: 0.780663, acc.: 52.34%] [G loss: 0.980908]\n",
            "180 [D loss: 0.816433, acc.: 51.17%] [G loss: 1.005170]\n",
            "181 [D loss: 0.800397, acc.: 52.93%] [G loss: 0.955896]\n",
            "182 [D loss: 0.791808, acc.: 52.93%] [G loss: 0.872588]\n",
            "183 [D loss: 0.776102, acc.: 53.71%] [G loss: 0.957563]\n",
            "184 [D loss: 0.782495, acc.: 52.54%] [G loss: 0.928655]\n",
            "185 [D loss: 0.783210, acc.: 52.73%] [G loss: 0.906727]\n",
            "186 [D loss: 0.793626, acc.: 52.93%] [G loss: 0.990595]\n",
            "187 [D loss: 0.785241, acc.: 54.10%] [G loss: 0.968803]\n",
            "188 [D loss: 0.789022, acc.: 51.95%] [G loss: 0.999437]\n",
            "189 [D loss: 0.841617, acc.: 47.27%] [G loss: 0.871988]\n",
            "190 [D loss: 0.811524, acc.: 50.98%] [G loss: 0.931688]\n",
            "191 [D loss: 0.789459, acc.: 53.32%] [G loss: 0.923091]\n",
            "192 [D loss: 0.784862, acc.: 52.93%] [G loss: 0.865506]\n",
            "193 [D loss: 0.818395, acc.: 50.78%] [G loss: 0.866293]\n",
            "194 [D loss: 0.802810, acc.: 52.34%] [G loss: 0.929209]\n",
            "195 [D loss: 0.798763, acc.: 51.95%] [G loss: 0.896652]\n",
            "196 [D loss: 0.804930, acc.: 50.20%] [G loss: 0.959187]\n",
            "197 [D loss: 0.754266, acc.: 53.32%] [G loss: 0.959102]\n",
            "198 [D loss: 0.847005, acc.: 47.66%] [G loss: 0.951305]\n",
            "199 [D loss: 0.796740, acc.: 51.17%] [G loss: 0.969538]\n",
            "200 [D loss: 0.796646, acc.: 53.32%] [G loss: 0.901409]\n",
            "201 [D loss: 0.777805, acc.: 53.32%] [G loss: 0.940648]\n",
            "202 [D loss: 0.771975, acc.: 53.32%] [G loss: 0.856500]\n",
            "203 [D loss: 0.752088, acc.: 57.81%] [G loss: 0.896787]\n",
            "204 [D loss: 0.766978, acc.: 52.34%] [G loss: 0.984596]\n",
            "205 [D loss: 0.767740, acc.: 54.30%] [G loss: 0.993610]\n",
            "206 [D loss: 0.774964, acc.: 53.52%] [G loss: 0.923845]\n",
            "207 [D loss: 0.761641, acc.: 54.69%] [G loss: 0.841388]\n",
            "208 [D loss: 0.757306, acc.: 54.10%] [G loss: 0.902111]\n",
            "209 [D loss: 0.784391, acc.: 54.88%] [G loss: 0.922295]\n",
            "210 [D loss: 0.806244, acc.: 51.17%] [G loss: 0.913360]\n",
            "211 [D loss: 0.805280, acc.: 49.80%] [G loss: 0.957539]\n",
            "212 [D loss: 0.772761, acc.: 50.78%] [G loss: 0.850660]\n",
            "213 [D loss: 0.798367, acc.: 52.34%] [G loss: 0.948611]\n",
            "214 [D loss: 0.799525, acc.: 51.95%] [G loss: 0.948689]\n",
            "215 [D loss: 0.784652, acc.: 52.54%] [G loss: 0.903280]\n",
            "216 [D loss: 0.779174, acc.: 54.10%] [G loss: 0.835000]\n",
            "217 [D loss: 0.813309, acc.: 48.63%] [G loss: 1.007115]\n",
            "218 [D loss: 0.846851, acc.: 51.95%] [G loss: 0.831972]\n",
            "219 [D loss: 0.818188, acc.: 54.88%] [G loss: 0.920589]\n",
            "220 [D loss: 0.806872, acc.: 53.91%] [G loss: 0.964684]\n",
            "221 [D loss: 0.822057, acc.: 47.85%] [G loss: 0.858217]\n",
            "222 [D loss: 0.816050, acc.: 52.15%] [G loss: 0.870997]\n",
            "223 [D loss: 0.782301, acc.: 53.91%] [G loss: 0.946556]\n",
            "224 [D loss: 0.799303, acc.: 50.00%] [G loss: 0.960627]\n",
            "225 [D loss: 0.779500, acc.: 52.93%] [G loss: 0.903814]\n",
            "226 [D loss: 0.819402, acc.: 47.85%] [G loss: 0.935969]\n",
            "227 [D loss: 0.806596, acc.: 52.15%] [G loss: 0.857619]\n",
            "228 [D loss: 0.759786, acc.: 56.05%] [G loss: 0.931618]\n",
            "229 [D loss: 0.781293, acc.: 50.78%] [G loss: 0.952704]\n",
            "230 [D loss: 0.800434, acc.: 53.32%] [G loss: 0.937655]\n",
            "231 [D loss: 0.780207, acc.: 54.10%] [G loss: 0.909767]\n",
            "232 [D loss: 0.798916, acc.: 51.76%] [G loss: 0.942855]\n",
            "233 [D loss: 0.789457, acc.: 54.69%] [G loss: 0.926328]\n",
            "234 [D loss: 0.785749, acc.: 53.52%] [G loss: 0.949203]\n",
            "235 [D loss: 0.794019, acc.: 53.71%] [G loss: 0.904678]\n",
            "236 [D loss: 0.819363, acc.: 51.37%] [G loss: 0.953824]\n",
            "237 [D loss: 0.840413, acc.: 51.76%] [G loss: 0.910328]\n",
            "238 [D loss: 0.834660, acc.: 46.68%] [G loss: 0.952292]\n",
            "239 [D loss: 0.771117, acc.: 55.08%] [G loss: 0.946496]\n",
            "240 [D loss: 0.788061, acc.: 53.71%] [G loss: 0.955980]\n",
            "241 [D loss: 0.765384, acc.: 53.12%] [G loss: 0.922072]\n",
            "242 [D loss: 0.768939, acc.: 53.12%] [G loss: 0.932501]\n",
            "243 [D loss: 0.762544, acc.: 54.69%] [G loss: 0.783535]\n",
            "244 [D loss: 0.771858, acc.: 53.32%] [G loss: 0.909504]\n",
            "245 [D loss: 0.815322, acc.: 51.17%] [G loss: 0.858738]\n",
            "246 [D loss: 0.820739, acc.: 51.37%] [G loss: 0.952358]\n",
            "247 [D loss: 0.820838, acc.: 49.41%] [G loss: 0.897370]\n",
            "248 [D loss: 0.763195, acc.: 55.86%] [G loss: 0.962670]\n",
            "249 [D loss: 0.832800, acc.: 50.59%] [G loss: 0.879529]\n",
            "250 [D loss: 0.787955, acc.: 51.95%] [G loss: 0.960663]\n",
            "251 [D loss: 0.834197, acc.: 50.20%] [G loss: 0.907090]\n",
            "252 [D loss: 0.811440, acc.: 53.52%] [G loss: 0.808150]\n",
            "253 [D loss: 0.820623, acc.: 49.22%] [G loss: 0.964636]\n",
            "254 [D loss: 0.807556, acc.: 49.61%] [G loss: 0.994355]\n",
            "255 [D loss: 0.782217, acc.: 51.17%] [G loss: 0.909064]\n",
            "256 [D loss: 0.772043, acc.: 52.73%] [G loss: 0.886649]\n",
            "257 [D loss: 0.779145, acc.: 55.86%] [G loss: 1.007739]\n",
            "258 [D loss: 0.832431, acc.: 49.61%] [G loss: 0.907586]\n",
            "259 [D loss: 0.785724, acc.: 53.91%] [G loss: 0.935726]\n",
            "260 [D loss: 0.778180, acc.: 53.71%] [G loss: 0.898475]\n",
            "261 [D loss: 0.796904, acc.: 53.91%] [G loss: 0.925197]\n",
            "262 [D loss: 0.805306, acc.: 51.37%] [G loss: 0.995128]\n",
            "263 [D loss: 0.788163, acc.: 52.93%] [G loss: 0.873070]\n",
            "264 [D loss: 0.800086, acc.: 51.17%] [G loss: 0.905305]\n",
            "265 [D loss: 0.809144, acc.: 51.17%] [G loss: 0.916510]\n",
            "266 [D loss: 0.806579, acc.: 51.56%] [G loss: 0.895838]\n",
            "267 [D loss: 0.769353, acc.: 52.34%] [G loss: 0.931730]\n",
            "268 [D loss: 0.779502, acc.: 52.54%] [G loss: 0.991450]\n",
            "269 [D loss: 0.840941, acc.: 50.39%] [G loss: 0.889735]\n",
            "270 [D loss: 0.760430, acc.: 54.69%] [G loss: 0.931162]\n",
            "271 [D loss: 0.784546, acc.: 49.41%] [G loss: 0.934541]\n",
            "272 [D loss: 0.825311, acc.: 50.00%] [G loss: 0.950664]\n",
            "273 [D loss: 0.790148, acc.: 50.20%] [G loss: 0.877639]\n",
            "274 [D loss: 0.754745, acc.: 55.27%] [G loss: 0.966041]\n",
            "275 [D loss: 0.816786, acc.: 52.15%] [G loss: 0.941259]\n",
            "276 [D loss: 0.767883, acc.: 52.54%] [G loss: 0.990766]\n",
            "277 [D loss: 0.781745, acc.: 52.54%] [G loss: 0.996251]\n",
            "278 [D loss: 0.783728, acc.: 54.30%] [G loss: 0.970019]\n",
            "279 [D loss: 0.763357, acc.: 53.32%] [G loss: 0.880515]\n",
            "280 [D loss: 0.799417, acc.: 51.17%] [G loss: 0.936704]\n",
            "281 [D loss: 0.798319, acc.: 53.52%] [G loss: 0.921059]\n",
            "282 [D loss: 0.785420, acc.: 52.54%] [G loss: 0.841803]\n",
            "283 [D loss: 0.787231, acc.: 52.15%] [G loss: 0.877060]\n",
            "284 [D loss: 0.783288, acc.: 53.91%] [G loss: 0.959031]\n",
            "285 [D loss: 0.800756, acc.: 50.98%] [G loss: 0.847430]\n",
            "286 [D loss: 0.800997, acc.: 53.32%] [G loss: 0.971372]\n",
            "287 [D loss: 0.790965, acc.: 54.69%] [G loss: 0.928834]\n",
            "288 [D loss: 0.766353, acc.: 57.03%] [G loss: 0.858238]\n",
            "289 [D loss: 0.790560, acc.: 51.37%] [G loss: 0.915169]\n",
            "290 [D loss: 0.763493, acc.: 56.05%] [G loss: 0.897495]\n",
            "291 [D loss: 0.809075, acc.: 50.39%] [G loss: 0.877941]\n",
            "292 [D loss: 0.762391, acc.: 56.64%] [G loss: 0.969001]\n",
            "293 [D loss: 0.807092, acc.: 50.20%] [G loss: 0.879699]\n",
            "294 [D loss: 0.867660, acc.: 44.73%] [G loss: 0.883322]\n",
            "295 [D loss: 0.827381, acc.: 49.61%] [G loss: 0.889871]\n",
            "296 [D loss: 0.777110, acc.: 53.32%] [G loss: 0.817235]\n",
            "297 [D loss: 0.819884, acc.: 48.44%] [G loss: 0.871818]\n",
            "298 [D loss: 0.799386, acc.: 52.34%] [G loss: 0.877689]\n",
            "299 [D loss: 0.796622, acc.: 50.00%] [G loss: 0.913694]\n",
            "300 [D loss: 0.781806, acc.: 55.47%] [G loss: 0.927241]\n",
            "301 [D loss: 0.792441, acc.: 50.98%] [G loss: 0.766172]\n",
            "302 [D loss: 0.800873, acc.: 51.76%] [G loss: 0.908870]\n",
            "303 [D loss: 0.806529, acc.: 52.73%] [G loss: 0.904485]\n",
            "304 [D loss: 0.780357, acc.: 51.17%] [G loss: 0.929141]\n",
            "305 [D loss: 0.826906, acc.: 49.61%] [G loss: 0.930971]\n",
            "306 [D loss: 0.782878, acc.: 51.37%] [G loss: 0.943865]\n",
            "307 [D loss: 0.808541, acc.: 51.37%] [G loss: 0.840927]\n",
            "308 [D loss: 0.755380, acc.: 55.66%] [G loss: 0.876562]\n",
            "309 [D loss: 0.789949, acc.: 50.59%] [G loss: 0.935421]\n",
            "310 [D loss: 0.749500, acc.: 56.05%] [G loss: 0.881310]\n",
            "311 [D loss: 0.813259, acc.: 51.17%] [G loss: 0.888027]\n",
            "312 [D loss: 0.810840, acc.: 52.15%] [G loss: 0.889340]\n",
            "313 [D loss: 0.790734, acc.: 51.95%] [G loss: 0.921870]\n",
            "314 [D loss: 0.800994, acc.: 50.00%] [G loss: 0.908034]\n",
            "315 [D loss: 0.766013, acc.: 54.10%] [G loss: 0.994685]\n",
            "316 [D loss: 0.751624, acc.: 54.88%] [G loss: 0.930220]\n",
            "317 [D loss: 0.805578, acc.: 52.93%] [G loss: 0.970701]\n",
            "318 [D loss: 0.768839, acc.: 55.27%] [G loss: 0.868632]\n",
            "319 [D loss: 0.771977, acc.: 54.30%] [G loss: 1.000887]\n",
            "320 [D loss: 0.799065, acc.: 52.54%] [G loss: 0.986350]\n",
            "321 [D loss: 0.811514, acc.: 52.15%] [G loss: 0.954483]\n",
            "322 [D loss: 0.796871, acc.: 54.10%] [G loss: 0.886211]\n",
            "323 [D loss: 0.776412, acc.: 53.52%] [G loss: 0.861519]\n",
            "324 [D loss: 0.785596, acc.: 52.73%] [G loss: 0.924753]\n",
            "325 [D loss: 0.760765, acc.: 53.91%] [G loss: 0.888668]\n",
            "326 [D loss: 0.811029, acc.: 54.88%] [G loss: 0.912343]\n",
            "327 [D loss: 0.764502, acc.: 52.93%] [G loss: 0.890898]\n",
            "328 [D loss: 0.783234, acc.: 53.52%] [G loss: 0.960827]\n",
            "329 [D loss: 0.786375, acc.: 51.95%] [G loss: 0.952572]\n",
            "330 [D loss: 0.810897, acc.: 51.76%] [G loss: 0.947800]\n",
            "331 [D loss: 0.797043, acc.: 52.15%] [G loss: 0.906629]\n",
            "332 [D loss: 0.819847, acc.: 50.59%] [G loss: 0.903761]\n",
            "333 [D loss: 0.804728, acc.: 51.76%] [G loss: 0.933777]\n",
            "334 [D loss: 0.738951, acc.: 57.62%] [G loss: 0.856929]\n",
            "335 [D loss: 0.782835, acc.: 53.71%] [G loss: 0.930541]\n",
            "336 [D loss: 0.772333, acc.: 53.71%] [G loss: 0.990981]\n",
            "337 [D loss: 0.809730, acc.: 50.59%] [G loss: 0.865435]\n",
            "338 [D loss: 0.824429, acc.: 51.37%] [G loss: 0.948345]\n",
            "339 [D loss: 0.833759, acc.: 50.39%] [G loss: 0.894219]\n",
            "340 [D loss: 0.772037, acc.: 50.59%] [G loss: 0.922174]\n",
            "341 [D loss: 0.792953, acc.: 51.56%] [G loss: 0.922989]\n",
            "342 [D loss: 0.758347, acc.: 54.88%] [G loss: 0.853908]\n",
            "343 [D loss: 0.810352, acc.: 50.98%] [G loss: 1.002634]\n",
            "344 [D loss: 0.818789, acc.: 50.59%] [G loss: 0.945781]\n",
            "345 [D loss: 0.752053, acc.: 54.69%] [G loss: 0.880705]\n",
            "346 [D loss: 0.792263, acc.: 53.12%] [G loss: 0.855228]\n",
            "347 [D loss: 0.747979, acc.: 54.69%] [G loss: 0.903959]\n",
            "348 [D loss: 0.778056, acc.: 54.10%] [G loss: 0.860525]\n",
            "349 [D loss: 0.809872, acc.: 50.39%] [G loss: 0.962623]\n",
            "350 [D loss: 0.790105, acc.: 53.12%] [G loss: 0.899815]\n",
            "351 [D loss: 0.783523, acc.: 55.86%] [G loss: 0.910834]\n",
            "352 [D loss: 0.763598, acc.: 55.47%] [G loss: 0.915606]\n",
            "353 [D loss: 0.815084, acc.: 49.22%] [G loss: 0.935458]\n",
            "354 [D loss: 0.778309, acc.: 52.93%] [G loss: 1.002512]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap-D_lBg4y6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzmRCMkB4y6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si98sbXo4y6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "    File = data.iloc[i]\n",
        "    plt.figure(i)\n",
        "    I = cv2.imread(File.imageFilename)\n",
        "    ## Extract Coordinates of Optic disc\n",
        "    x = int(File['x'])\n",
        "    y = int(File['y'])\n",
        "    x_width = int(File['x_width'])\n",
        "    y_width = int(File['y_width'])\n",
        "    ## Extract patch\n",
        "    I = I[x:x+x_width,y:y+y_width,:]\n",
        "    I = cv2.resize(I, (224, 224))\n",
        "    I = cv2.normalize(I, None, alpha=0,  # Normalize image to fit from 0 to 1\n",
        "                                  beta=1,\n",
        "                                  norm_type=cv2.NORM_MINMAX,\n",
        "                                  dtype=cv2.CV_32F)\n",
        "    I = cv2.cvtColor(I,4)\n",
        "    plt.imshow(I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W7U-Spq4y6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjAKKv0V4y60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJYpwvO4y64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}